{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('train.tsv', sep='\\t')\n",
    "\n",
    "df = df [:3000]\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "text_data = train[\"Phrase\"]\n",
    "text_data = vectorizer.fit_transform(text_data)\n",
    "\n",
    "text_tensor_train = torch.from_numpy(text_data.todense()).float()\n",
    "\n",
    "text_tensor_train = text_tensor_train.unsqueeze(1)\n",
    "\n",
    "#ajout du cas où on ne connait pas le mot\n",
    "voca = vectorizer.get_feature_names() #+ list(['<unk>'])\n",
    "#ajout d'un zéro à la fin de chaque tensor\n",
    "# unk = torch.zeros([text_tensor_train.shape[0],1], dtype=torch.float)\n",
    "# text_tensor_train = torch.cat((text_tensor_train, unk), 1).unsqueeze(1)\n",
    "\n",
    "def labelToVec(label):\n",
    "    # label_one_hot_vector = torch.tensor(pd.get_dummies(df[\"Sentiment\"]).values)\n",
    "    label_tensor = torch.tensor(label[\"Sentiment\"].values)\n",
    "    return label_tensor\n",
    "\n",
    "label_tensor_train = labelToVec(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def vocaToUnk(sentence, voca):\n",
    "    s = []\n",
    "    for w in word_tokenize(sentence):\n",
    "        if(w not in voca):\n",
    "            w = '<unk>'\n",
    "        s.append(w)\n",
    "    return \" \".join(s)         \n",
    "                \n",
    "# text_test = test.Phrase.apply(lambda x : vocaToUnk(x, voca))\n",
    "#Ne prend pas en compte <unk>\n",
    "text_test = vectorizer.transform(test[\"Phrase\"])\n",
    "\n",
    "text_tensor_test = torch.from_numpy(text_test.todense()).float()\n",
    "label_tensor_test = labelToVec(test)\n",
    "\n",
    "text_tensor_test = text_tensor_test.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(text_tensor_test[1], text_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        self.rnn = nn.RNN(input_size = input_size, hidden_size = hidden_size, batch_first = True)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)       \n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        hidden = hidden.to(device)\n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "        output = self.h2o(hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(voca)\n",
    "embedding_size = 100\n",
    "hidden_dim = 100\n",
    "output_dim = 5\n",
    "\n",
    "rnn = RNN(input_dim, embedding_size, hidden_dim, output_dim)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=1e-3)\n",
    "\n",
    "def trainRNN(train_category, train_text, test_category, test_text, num_epoch, batch_size):\n",
    "    \n",
    "    train_category = train_category.to(device)\n",
    "    train_text = train_text.to(device)\n",
    "    test_category = test_category.to(device)\n",
    "    test_text = test_text.to(device)\n",
    "    \n",
    "    size_train = train_text.size(0)\n",
    "    size_test = test_text.size(0)\n",
    "    \n",
    "    hidden = rnn.initHidden(batch_size)   \n",
    "    criterion = nn.CrossEntropyLoss()    \n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    total_pred = torch.tensor([], dtype = torch.long)\n",
    "    total_targ = torch.tensor([], dtype = torch.long)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        #un jour je ferais un truc propre\n",
    "        nb_batch_train = len(train_text) / batch_size\n",
    "        nb_batch_test = len(test_text) / batch_size\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        i = 0\n",
    "        while (i + batch_size) <= size_train:\n",
    "            tmp = i\n",
    "            i += batch_size\n",
    "            input = train_text[tmp:i]\n",
    "            target = train_category[tmp:i]\n",
    "            output, hidden = rnn(input, hidden)\n",
    "            \n",
    "            loss = criterion(output.squeeze(0), target)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step() \n",
    "            \n",
    "            predicted = torch.argmax(output.data, dim=2)\n",
    "            correct = (predicted == target).sum().item()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_acc += correct / batch_size\n",
    "        \n",
    "        i = 0\n",
    "        test_loss = 0\n",
    "        test_acc = 0        \n",
    "        \n",
    "        #j'aime faire des boucles presque pareilles\n",
    "        while (i + batch_size) <= size_test:\n",
    "            tmp = i\n",
    "            i += batch_size\n",
    "            input = test_text[tmp:i]\n",
    "            target = test_category[tmp:i]\n",
    "            \n",
    "            output, _ = rnn(input, hidden)\n",
    "            loss = criterion(output.squeeze(0), target)   \n",
    "            \n",
    "            predicted = torch.argmax(output.data, dim=2)\n",
    "            correct = (predicted == target).sum().item()\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            test_acc += correct / batch_size\n",
    "            \n",
    "            #dernière epoch\n",
    "            if epoch + 1 == num_epoch:\n",
    "                total_pred = torch.cat((total_pred, predicted), 1)\n",
    "                total_targ = torch.cat((total_targ, target), 0)\n",
    "                \n",
    "        print(epoch, \"loss :\", train_loss / nb_batch_train, \"/ acc :\", train_acc / nb_batch_train)\n",
    "        print(\"Test loss :\", test_loss / nb_batch_test, \"/ acc :\", test_acc / nb_batch_test)\n",
    "        \n",
    "    print('Fini !')\n",
    "    \n",
    "    return total_pred, total_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss : 1.397036850452423 / acc : 0.5291666666666668\n",
      "Test loss : 1.2240630934635799 / acc : 0.585\n",
      "1 loss : 1.211579106748104 / acc : 0.6125\n",
      "Test loss : 1.2347112099329631 / acc : 0.5866666666666667\n",
      "2 loss : 1.126040695856015 / acc : 0.61375\n",
      "Test loss : 1.236247147123019 / acc : 0.585\n",
      "3 loss : 1.1351089005668957 / acc : 0.6120833333333334\n",
      "Test loss : 1.2770091990629833 / acc : 0.5249999999999999\n",
      "4 loss : 1.5957910145322483 / acc : 0.5175\n",
      "Test loss : 1.4466505646705627 / acc : 0.585\n",
      "Fini !\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "nb_epoch = 5\n",
    "\n",
    "pred, real = trainRNN(label_tensor_train, text_tensor_train, label_tensor_test, text_tensor_test, nb_epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "0      71\n",
      "1     384\n",
      "2    1470\n",
      "3     368\n",
      "4     107\n",
      "Name: Sentiment, dtype: int64\n",
      "Sentiment\n",
      "0     22\n",
      "1     93\n",
      "2    351\n",
      "3     95\n",
      "4     39\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noadkoko/.local/lib/python3.6/site-packages/pandas_ml/confusion_matrix/abstract.py:66: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  df = df.loc[idx, idx.copy()].fillna(0)  # if some columns or rows are missing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,  22,   0,   0],\n",
       "       [  0,   0,  93,   0,   0],\n",
       "       [  0,   0, 351,   0,   0],\n",
       "       [  0,   0,  95,   0,   0],\n",
       "       [  0,   0,  39,   0,   0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from pandas_ml import ConfusionMatrix\n",
    "\n",
    "print(train.groupby('Sentiment')[\"Sentiment\"].count())\n",
    "print(test.groupby('Sentiment')[\"Sentiment\"].count())\n",
    "\n",
    "#warning chiante, mais np\n",
    "cm = ConfusionMatrix(real.data, pred.squeeze(0).data)\n",
    "confusion_matrix(real, pred.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
